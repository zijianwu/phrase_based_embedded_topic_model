{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:58:20.309552Z",
     "start_time": "2020-07-26T02:58:20.305572Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:58:20.705767Z",
     "start_time": "2020-07-26T02:58:20.703079Z"
    }
   },
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:58:21.733808Z",
     "start_time": "2020-07-26T02:58:21.728244Z"
    }
   },
   "outputs": [],
   "source": [
    "max_df = 0.7\n",
    "min_df = 10  # choose desired value for min_df\n",
    "\n",
    "all_paths = os.listdir(f\"{current_path}/../data/processed/20_newsgroups\")\n",
    "all_paths = [ele for ele in all_paths if '.txt' in ele]\n",
    "train_paths = [ele for ele in all_paths if 'train' in ele]\n",
    "test_paths = [ele for ele in all_paths if 'test' in ele]\n",
    "preprocessing_types = [ele[5:][:-4] for ele in test_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:58:22.311942Z",
     "start_time": "2020-07-26T02:58:22.304789Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_empty(in_docs):\n",
    "    return [doc for doc in in_docs if doc!=[]]\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T03:00:08.166321Z",
     "start_time": "2020-07-26T02:58:23.859843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 18446\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 16873\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  2054975\n",
      "  len(words_ts):  1335875\n",
      "  len(words_ts_h1):  666065\n",
      "  len(words_ts_h2):  669810\n",
      "  len(words_va):  18606\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 18295\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 16650\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  1371798\n",
      "  len(words_ts):  892353\n",
      "  len(words_ts_h1):  444295\n",
      "  len(words_ts_h2):  448058\n",
      "  len(words_va):  18404\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 21361\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 19703\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  1368862\n",
      "  len(words_ts):  884143\n",
      "  len(words_ts_h1):  440185\n",
      "  len(words_ts_h2):  443958\n",
      "  len(words_va):  9716\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 21549\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 19982\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  2157411\n",
      "  len(words_ts):  1403967\n",
      "  len(words_ts_h1):  700116\n",
      "  len(words_ts_h2):  703851\n",
      "  len(words_va):  21607\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 18449\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 16878\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  2050710\n",
      "  len(words_ts):  1335837\n",
      "  len(words_ts_h1):  666045\n",
      "  len(words_ts_h2):  669792\n",
      "  len(words_va):  22784\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 18298\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 16659\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  1375560\n",
      "  len(words_ts):  892305\n",
      "  len(words_ts_h1):  444263\n",
      "  len(words_ts_h2):  448042\n",
      "  len(words_va):  14565\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 171\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 3\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  48\n",
      "  len(words_ts):  0\n",
      "  len(words_ts_h1):  0\n",
      "  len(words_ts_h2):  0\n",
      "  len(words_va):  0\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 48 [this should be 48]\n",
      "  len(np.unique(doc_indices_ts)): 0 [this should be 0]\n",
      "  len(np.unique(doc_indices_ts_h1)): 0 [this should be 0]\n",
      "  len(np.unique(doc_indices_ts_h2)): 0 [this should be 0]\n",
      "  len(np.unique(doc_indices_va)): 0 [this should be 0]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n",
      "building the vocabulary...\n",
      "  initial vocabulary size: 174\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 6\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  113\n",
      "  len(words_ts):  2\n",
      "  len(words_ts_h1):  1\n",
      "  len(words_ts_h2):  1\n",
      "  len(words_va):  1\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 108 [this should be 108]\n",
      "  len(np.unique(doc_indices_ts)): 1 [this should be 1]\n",
      "  len(np.unique(doc_indices_ts_h1)): 1 [this should be 1]\n",
      "  len(np.unique(doc_indices_ts_h2)): 1 [this should be 1]\n",
      "  len(np.unique(doc_indices_va)): 1 [this should be 1]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n",
      "counting document frequency of words...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the vocabulary...\n",
      "  initial vocabulary size: 21361\n",
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 19701\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n",
      "splitting test documents in 2 halves...\n",
      "creating lists of words...\n",
      "  len(words_tr):  1367957\n",
      "  len(words_ts):  884134\n",
      "  len(words_ts_h1):  440178\n",
      "  len(words_ts_h2):  443956\n",
      "  len(words_va):  10618\n",
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7532 [this should be 7532]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n",
      "creating bow representation...\n",
      "splitting bow intro token/value pairs and saving to disk...\n",
      "Data ready !!\n"
     ]
    }
   ],
   "source": [
    "for preprocessing_type in preprocessing_types:\n",
    "    train_file = open(f\"{current_path}/../data/processed/20_newsgroups/train_{preprocessing_type}.txt\", \"r\")\n",
    "    init_docs_tr = train_file.readlines()\n",
    "    train_file.close()\n",
    "\n",
    "    test_file = open(f\"{current_path}/../data/processed/20_newsgroups/test_{preprocessing_type}.txt\", \"r\")\n",
    "    init_docs_ts = test_file.readlines()\n",
    "    test_file.close()\n",
    "    \n",
    "    init_docs = init_docs_tr + init_docs_ts\n",
    "    \n",
    "    # Create count vectorizer\n",
    "    print('counting document frequency of words...')\n",
    "    cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None)\n",
    "    cvz = cvectorizer.fit_transform(init_docs).sign()\n",
    "\n",
    "    # Get vocabulary\n",
    "    print('building the vocabulary...')\n",
    "    sum_counts = cvz.sum(axis=0)\n",
    "    v_size = sum_counts.shape[1]\n",
    "    sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "    for v in range(v_size):\n",
    "        sum_counts_np[v] = sum_counts[0,v]\n",
    "    word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\n",
    "    id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n",
    "    del cvectorizer\n",
    "    print('  initial vocabulary size: {}'.format(v_size))\n",
    "\n",
    "    # Sort elements in vocabulary\n",
    "    idx_sort = np.argsort(sum_counts_np)\n",
    "    vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]\n",
    "    \n",
    "    # Create dictionary and inverse dictionary\n",
    "    vocab = vocab_aux\n",
    "    del vocab_aux\n",
    "    word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "    id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "\n",
    "    # Split in train/test/valid\n",
    "    print('tokenizing documents and splitting into train/test/valid...')\n",
    "    num_docs_tr = len(init_docs_tr)\n",
    "    trSize = num_docs_tr-100\n",
    "    tsSize = len(init_docs_ts)\n",
    "    vaSize = 100\n",
    "    idx_permute = np.random.permutation(num_docs_tr).astype(int)\n",
    "\n",
    "    # Remove words not in train_data\n",
    "    vocab = list(set([w for idx_d in range(trSize) for w in init_docs[idx_permute[idx_d]].split() if w in word2id]))\n",
    "    word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "    id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "    print('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n",
    "\n",
    "    # Split in train/test/valid\n",
    "    docs_tr = [[word2id[w] for w in init_docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "    docs_va = [[word2id[w] for w in init_docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "    docs_ts = [[word2id[w] for w in init_docs[idx_d+num_docs_tr].split() if w in word2id] for idx_d in range(tsSize)]\n",
    "\n",
    "    print('  number of documents (train): {} [this should be equal to {}]'.format(len(docs_tr), trSize))\n",
    "    print('  number of documents (test): {} [this should be equal to {}]'.format(len(docs_ts), tsSize))\n",
    "    print('  number of documents (valid): {} [this should be equal to {}]'.format(len(docs_va), vaSize))\n",
    "\n",
    "    # Remove empty documents\n",
    "    print('removing empty documents...')\n",
    "\n",
    "\n",
    "\n",
    "    docs_tr = remove_empty(docs_tr)\n",
    "    docs_ts = remove_empty(docs_ts)\n",
    "    docs_va = remove_empty(docs_va)\n",
    "\n",
    "    # Remove test documents with length=1\n",
    "    docs_ts = [doc for doc in docs_ts if len(doc)>1]\n",
    "\n",
    "    # Split test set in 2 halves\n",
    "    print('splitting test documents in 2 halves...')\n",
    "    docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "    docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]\n",
    "\n",
    "    # Getting lists of words and doc_indices\n",
    "    print('creating lists of words...')\n",
    "    \n",
    "    words_tr = create_list_words(docs_tr)\n",
    "    words_ts = create_list_words(docs_ts)\n",
    "    words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "    words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "    words_va = create_list_words(docs_va)\n",
    "\n",
    "    print('  len(words_tr): ', len(words_tr))\n",
    "    print('  len(words_ts): ', len(words_ts))\n",
    "    print('  len(words_ts_h1): ', len(words_ts_h1))\n",
    "    print('  len(words_ts_h2): ', len(words_ts_h2))\n",
    "    print('  len(words_va): ', len(words_va))\n",
    "\n",
    "    # Get doc indices\n",
    "    print('getting doc indices...')\n",
    "\n",
    "\n",
    "\n",
    "    doc_indices_tr = create_doc_indices(docs_tr)\n",
    "    doc_indices_ts = create_doc_indices(docs_ts)\n",
    "    doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "    doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "    doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "    print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "    print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "    print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "    print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "    print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))\n",
    "\n",
    "    # Number of documents in each set\n",
    "    n_docs_tr = len(docs_tr)\n",
    "    n_docs_ts = len(docs_ts)\n",
    "    n_docs_ts_h1 = len(docs_ts_h1)\n",
    "    n_docs_ts_h2 = len(docs_ts_h2)\n",
    "    n_docs_va = len(docs_va)\n",
    "\n",
    "    # Remove unused variables\n",
    "    del docs_tr\n",
    "    del docs_ts\n",
    "    del docs_ts_h1\n",
    "    del docs_ts_h2\n",
    "    del docs_va\n",
    "\n",
    "    # Create bow representation\n",
    "    print('creating bow representation...')\n",
    "\n",
    "\n",
    "\n",
    "    bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "    bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "    bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "    bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "    bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n",
    "\n",
    "    del words_tr\n",
    "    del words_ts\n",
    "    del words_ts_h1\n",
    "    del words_ts_h2\n",
    "    del words_va\n",
    "    del doc_indices_tr\n",
    "    del doc_indices_ts\n",
    "    del doc_indices_ts_h1\n",
    "    del doc_indices_ts_h2\n",
    "    del doc_indices_va\n",
    "    \n",
    "    # Write the vocabulary to a file\n",
    "    path_save = f\"{current_path}/../data/modeling/20_newsgroups/{preprocessing_type}/\"\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "\n",
    "    with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    del vocab\n",
    "\n",
    "    # Split bow intro token/value pairs\n",
    "    print('splitting bow intro token/value pairs and saving to disk...')\n",
    "\n",
    "\n",
    "\n",
    "    bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "    savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "    savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "    del bow_tr\n",
    "    del bow_tr_tokens\n",
    "    del bow_tr_counts\n",
    "\n",
    "    bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "    savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "    savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "    del bow_ts\n",
    "    del bow_ts_tokens\n",
    "    del bow_ts_counts\n",
    "\n",
    "    bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "    savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "    savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "    del bow_ts_h1\n",
    "    del bow_ts_h1_tokens\n",
    "    del bow_ts_h1_counts\n",
    "\n",
    "    bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "    savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "    savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "    del bow_ts_h2\n",
    "    del bow_ts_h2_tokens\n",
    "    del bow_ts_h2_counts\n",
    "\n",
    "    bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "    savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "    savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\n",
    "    del bow_va\n",
    "    del bow_va_tokens\n",
    "    del bow_va_counts\n",
    "\n",
    "    print('Data ready !!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
