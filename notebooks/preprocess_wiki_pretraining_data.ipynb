{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:22:33.073757Z",
     "start_time": "2020-07-16T22:22:31.456528Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from spacy.util import minibatch\n",
    "from spacy.attrs import LEMMA\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "import itertools \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\",\n",
    "    # \"parser\",\n",
    "    # \"ner\"\n",
    "])\n",
    "\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:22:33.506936Z",
     "start_time": "2020-07-16T22:22:33.504051Z"
    }
   },
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:23:08.567966Z",
     "start_time": "2020-07-16T22:23:08.470253Z"
    },
    "code_folding": [
     0,
     34,
     39,
     52,
     57,
     70,
     75,
     88,
     93,
     106,
     124,
     127,
     130,
     157,
     175,
     208,
     227,
     241
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_apply_list(a_list, a_function, n_jobs=mp.cpu_count(), func_param=None, n_threads=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies a_function to a_list using multiprocessing with n_jobs. If a_function has a specific\n",
    "    parameter that elements in a_list should fill, indicate it with func_param. If there are\n",
    "    other parameters in a_function that should be statically filled, use **kwargs.\n",
    "\n",
    "    If elements in a_list are tuples, lists, or anything else that provides multiple inputs\n",
    "    to a_function, wrap a_function so that it takes the entire tuple or list (see parallel_apply_row for example)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_list : list\n",
    "    a_function : function object\n",
    "    n_jobs : int (multiprocessing)\n",
    "    n_threads : int (threading)\n",
    "    func_param : None (defaults to first parameter in function) or str (parameter in a_function)\n",
    "    kwargs : static keyword arguments to be given to all instances of a_function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "    \"\"\"\n",
    "    if n_jobs:\n",
    "        executor = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "    else:\n",
    "        executor = Parallel(n_jobs=n_threads, backend=\"threading\", prefer=\"threads\")\n",
    "    do = delayed(partial(a_function, **kwargs))\n",
    "    if func_param:\n",
    "        tasks = (do(**{func_param: ele}) for ele in tqdm(a_list))\n",
    "    else:\n",
    "        tasks = (do(ele) for ele in tqdm(a_list))\n",
    "    result = executor(tasks)\n",
    "    return result\n",
    "\n",
    "def spacy_norm(text):\n",
    "    doc = nlp(text)\n",
    "    tokenized_doc = [ele.text.lower() for ele in doc if (not ele.is_space) and (not ele.is_punct)]\n",
    "    result = ' '.join(tokenized_doc) + '\\n'\n",
    "    return result\n",
    "def process_save_norm(texts, data_name, data_type, current_path):\n",
    "    norm_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_norm.txt\"\n",
    "    if os.path.exists(norm_outpath):\n",
    "        norm_outpath_write = 'a'\n",
    "    else:\n",
    "        norm_outpath_write = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(norm_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_norm)\n",
    "    f0 = open(norm_outpath, norm_outpath_write)\n",
    "    f0.writelines(results)\n",
    "    f0.close()\n",
    "\n",
    "def spacy_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ele.lemma_.lower() for ele in doc if (not ele.is_space) and (not ele.is_punct)]\n",
    "    results = ' '.join(results) + '\\n'\n",
    "    return results\n",
    "def process_save_lemma(texts, data_name, data_type, current_path):\n",
    "    lemma_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_lemma.txt\"\n",
    "    if os.path.exists(lemma_outpath):\n",
    "        lemma_outpath_write = 'a'\n",
    "    else:\n",
    "        lemma_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(lemma_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_lemma)\n",
    "    f1 = open(lemma_outpath, lemma_outpath_write)\n",
    "    f1.writelines(results)\n",
    "    f1.close()       \n",
    "\n",
    "def spacy_no_stop(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ele.text.lower() for ele in doc if ((not ele.is_space) and (not ele.is_punct) and (not ele.is_stop))]\n",
    "    results = ' '.join(results) + '\\n'\n",
    "    return results\n",
    "def process_save_no_stop(texts, data_name, data_type, current_path):\n",
    "    no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_no_stop.txt\"\n",
    "    if os.path.exists(no_stop_outpath):\n",
    "        no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        no_stop_outpath_write = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(no_stop_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_no_stop)\n",
    "    f2 = open(no_stop_outpath, no_stop_outpath_write)\n",
    "    f2.writelines(results)\n",
    "    f2.close()\n",
    "    \n",
    "def spacy_lemma_no_stop(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ele.lemma_.lower() for ele in doc if ((not ele.is_space) and (not ele.is_punct) and (not ele.is_stop))]\n",
    "    results = ' '.join(results) + '\\n'\n",
    "    return results\n",
    "def process_save_lemma_no_stop(texts, data_name, data_type, current_path):\n",
    "    lemma_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_lemma_no_stop.txt\"\n",
    "    if os.path.exists(lemma_no_stop_outpath):\n",
    "        lemma_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        lemma_no_stop_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(lemma_no_stop_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_lemma_no_stop)\n",
    "    f3 = open(lemma_no_stop_outpath, lemma_no_stop_outpath_write)\n",
    "    f3.writelines(results)\n",
    "    f3.close()\n",
    "\n",
    "def spacy_np_lemma(text):\n",
    "    doc = nlp(text.replace('\\n', ' '))\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.lemma_.lower().replace(' ', '_') for ele in doc if ((not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "def extract_noun_phrases_from_tok_list(a_list_of_toks):\n",
    "    results = [ele for ele in a_list_of_toks if '_' in ele]\n",
    "    return results\n",
    "def join_list_of_strings_add_newline(a_list_of_toks):\n",
    "    result = ' '.join(a_list_of_toks) + '\\n'\n",
    "    return result\n",
    "def process_save_np_lemma(texts, data_name, data_type, current_path):\n",
    "    np_lemma_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma.txt\"\n",
    "    if os.path.exists(np_lemma_outpath):\n",
    "        np_lemma_outpath_write = 'a'\n",
    "    else:\n",
    "        np_lemma_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_np_lemma)\n",
    "    result1 = parallel_apply_list(results, join_list_of_strings_add_newline) \n",
    "    f5 = open(np_lemma_outpath, np_lemma_outpath_write)\n",
    "    f5.writelines(result1)\n",
    "    f5.close()\n",
    "    \n",
    "    np_lemma_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_only.txt\"\n",
    "    if os.path.exists(np_lemma_outpath_only):\n",
    "        np_lemma_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_lemma_outpath_write_only = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_outpath_only), exist_ok=True)\n",
    "    result2 = parallel_apply_list(results, extract_noun_phrases_from_tok_list)\n",
    "    result2 = [' '.join(ele) + '\\n' for ele in result2]\n",
    "    f8 = open(np_lemma_outpath_only, np_lemma_outpath_write_only)\n",
    "    f8.writelines(result2)\n",
    "    f8.close()\n",
    "    \n",
    "def spacy_np_no_stop(text):\n",
    "    doc = nlp(text.replace('\\n', ' '))\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.text.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.text.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.text.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.text.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.text.lower().strip().replace(' ', '_') for ele in doc if ((not ele.is_stop) and (not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "def process_save_np_no_stop(texts, data_name, data_type, current_path):\n",
    "    np_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_no_stop.txt\"\n",
    "    if os.path.exists(np_no_stop_outpath):\n",
    "        np_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        np_no_stop_outpath_write = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(np_no_stop_outpath), exist_ok=True)\n",
    "    start_time = time.time()\n",
    "    results = parallel_apply_list(texts, spacy_np_no_stop)\n",
    "    print(f\"        Finished spacy_np_no_stop, taking {int(time.time() - start_time)} seconds...\")\n",
    "    start_time = time.time()\n",
    "    result1 = parallel_apply_list(results, join_list_of_strings_add_newline) \n",
    "    print(f\"        Finished join_list_of_strings_add_newline, taking {int(time.time() - start_time)} seconds...\")\n",
    "    start_time = time.time()\n",
    "    f4 = open(np_no_stop_outpath, np_no_stop_outpath_write)\n",
    "    f4.writelines(result1)\n",
    "    f4.close()\n",
    "    print(f\"        Finished writing to file, taking {int(time.time() - start_time)} seconds...\")\n",
    "    \n",
    "    np_no_stop_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_no_stop_only.txt\"\n",
    "    if os.path.exists(np_no_stop_outpath_only):\n",
    "        np_no_stop_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_no_stop_outpath_write_only = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(np_no_stop_outpath_only), exist_ok=True)\n",
    "    result2 = parallel_apply_list(results, extract_noun_phrases_from_tok_list)\n",
    "    result2 = [' '.join(ele) + '\\n' for ele in result2]\n",
    "    f7 = open(np_no_stop_outpath_only, np_no_stop_outpath_write_only)\n",
    "    f7.writelines(result2)\n",
    "    f7.close()\n",
    "     \n",
    "def spacy_np_lemma_no_stop(text):\n",
    "    doc = nlp(text.replace('\\n', ' '))\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.lemma_.lower().replace(' ', '_') for ele in doc if ((not ele.is_stop) and (not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    tokenized_doc = ' '.join(tokenized_doc) + '\\n'\n",
    "    return tokenized_doc\n",
    "def process_save_np_lemma_no_stop(texts, data_name, data_type, current_path):\n",
    "    np_lemma_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_no_stop.txt\"\n",
    "    if os.path.exists(np_lemma_no_stop_outpath):\n",
    "        np_lemma_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        np_lemma_no_stop_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_no_stop_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_np_lemma_no_stop)  \n",
    "    f6 = open(np_lemma_no_stop_outpath, np_lemma_no_stop_outpath_write)           \n",
    "    f6.writelines(results)\n",
    "    f6.close()\n",
    "    \n",
    "    \n",
    "def fully_process_partitions(partition, data_name='20_newgroups', data_type='train', current_path=current_path):\n",
    "    print(\"    Processing norm...\")\n",
    "    process_save_norm(texts=partition,\n",
    "                      data_name=data_name,\n",
    "                      data_type=data_type,\n",
    "                      current_path=current_path)\n",
    "    print(\"    Processing lemma...\")\n",
    "    process_save_lemma(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing no_stop...\")\n",
    "    process_save_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing lemma_no_stop...\")\n",
    "    process_save_lemma_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing np_lemma...\")\n",
    "    process_save_np_lemma(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing np_no_stop...\")\n",
    "    process_save_np_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing np_lemma_no_stop...\")\n",
    "    process_save_np_lemma_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    \n",
    "    \n",
    "def grouper(n, iterable):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        chunk = [ele['section_texts'] for ele in chunk]\n",
    "        chunk = [ele for sublist in chunk for ele in sublist]\n",
    "        chunk = [ele if len(ele) > 999998 else ele[:999998] for ele in chunk]\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:23:12.996594Z",
     "start_time": "2020-07-16T22:23:12.929549Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = api.load('wiki-english-20171001')\n",
    "partitions = grouper(500, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T22:27:49.112570Z",
     "start_time": "2020-07-16T22:23:13.681162Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4753 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting partition 0\n",
      "    Processing norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4753/4753 [00:39<00:00, 119.17it/s]\n",
      "  0%|          | 0/4753 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4753/4753 [00:44<00:00, 105.66it/s]\n",
      "  0%|          | 0/4753 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4753/4753 [00:46<00:00, 102.43it/s]\n",
      "  0%|          | 0/4753 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4753/4753 [00:44<00:00, 105.96it/s]\n",
      "  0%|          | 0/4753 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4753/4753 [00:54<00:00, 86.61it/s] \n",
      "100%|██████████| 4753/4753 [00:00<00:00, 11536.41it/s]\n",
      "100%|██████████| 4753/4753 [00:00<00:00, 12304.54it/s]\n",
      "  0%|          | 0/4753 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2088/4753 [00:23<00:23, 113.92it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-76d76e975405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting partition {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfully_process_partitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wikipedia'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-34f069481366>\u001b[0m in \u001b[0;36mfully_process_partitions\u001b[0;34m(partition, data_name, data_type, current_path)\u001b[0m\n\u001b[1;32m    270\u001b[0m                        \u001b[0mdata_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                        \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                        current_path=current_path)\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"    Processing np_lemma_no_stop...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     process_save_np_lemma_no_stop(texts=partition,\n",
      "\u001b[0;32m<ipython-input-7-34f069481366>\u001b[0m in \u001b[0;36mprocess_save_np_no_stop\u001b[0;34m(texts, data_name, data_type, current_path)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_no_stop_outpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_apply_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy_np_no_stop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"        Finished spacy_np_no_stop, taking {int(time.time() - start_time)} seconds...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-34f069481366>\u001b[0m in \u001b[0;36mparallel_apply_list\u001b[0;34m(a_list, a_function, n_jobs, func_param, n_threads, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Starting partition {i}\")\n",
    "    fully_process_partitions(partition, data_name='wikipedia', data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
