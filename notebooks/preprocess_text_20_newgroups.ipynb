{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:00:30.724444Z",
     "start_time": "2020-07-25T00:00:30.152619Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from spacy.util import minibatch\n",
    "from spacy.attrs import LEMMA\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[#\"tagger\",\n",
    "    #\"parser\",\n",
    "    #\"ner\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:40:50.210478Z",
     "start_time": "2020-07-24T23:40:50.207273Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "newsgroups_train_data_loc = f\"{current_path}/../data/raw/20_newsgroups/train_data.pkl\"\n",
    "newsgroups_test_data_loc = f\"{current_path}/../data/raw/20_newsgroups/test_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:00:31.755000Z",
     "start_time": "2020-07-25T00:00:31.731664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "for ele in nlp(\"Autonomous cars shift insurance liability toward manufacturers.\").noun_chunks:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:41:03.683921Z",
     "start_time": "2020-07-24T23:41:03.597299Z"
    },
    "code_folding": [
     0,
     34,
     39,
     52,
     57,
     70,
     75,
     88,
     93,
     124,
     127,
     130,
     157,
     175,
     208,
     227,
     241
    ]
   },
   "outputs": [],
   "source": [
    "def parallel_apply_list(a_list, a_function, n_jobs=mp.cpu_count(), func_param=None, n_threads=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies a_function to a_list using multiprocessing with n_jobs. If a_function has a specific\n",
    "    parameter that elements in a_list should fill, indicate it with func_param. If there are\n",
    "    other parameters in a_function that should be statically filled, use **kwargs.\n",
    "\n",
    "    If elements in a_list are tuples, lists, or anything else that provides multiple inputs\n",
    "    to a_function, wrap a_function so that it takes the entire tuple or list (see parallel_apply_row for example)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_list : list\n",
    "    a_function : function object\n",
    "    n_jobs : int (multiprocessing)\n",
    "    n_threads : int (threading)\n",
    "    func_param : None (defaults to first parameter in function) or str (parameter in a_function)\n",
    "    kwargs : static keyword arguments to be given to all instances of a_function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "    \"\"\"\n",
    "    if n_jobs:\n",
    "        executor = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "    else:\n",
    "        executor = Parallel(n_jobs=n_threads, backend=\"threading\", prefer=\"threads\")\n",
    "    do = delayed(partial(a_function, **kwargs))\n",
    "    if func_param:\n",
    "        tasks = (do(**{func_param: ele}) for ele in tqdm(a_list))\n",
    "    else:\n",
    "        tasks = (do(ele) for ele in tqdm(a_list))\n",
    "    result = executor(tasks)\n",
    "    return result\n",
    "\n",
    "def spacy_norm(text):\n",
    "    doc = nlp(text)\n",
    "    tokenized_doc = [ele.text.lower() for ele in doc if (not ele.is_space) and (not ele.is_punct)]\n",
    "    result = ' '.join(tokenized_doc) + '\\n'\n",
    "    return result\n",
    "def process_save_norm(texts, data_name, data_type, current_path):\n",
    "    norm_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_norm.txt\"\n",
    "    if os.path.exists(norm_outpath):\n",
    "        norm_outpath_write = 'a'\n",
    "    else:\n",
    "        norm_outpath_write = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(norm_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_norm)\n",
    "    f0 = open(norm_outpath, norm_outpath_write)\n",
    "    f0.writelines(results)\n",
    "    f0.close()\n",
    "\n",
    "def spacy_lemma(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ele.lemma_.lower() for ele in doc if (not ele.is_space) and (not ele.is_punct)]\n",
    "    results = ' '.join(results) + '\\n'\n",
    "    return results\n",
    "def process_save_lemma(texts, data_name, data_type, current_path):\n",
    "    lemma_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_lemma.txt\"\n",
    "    if os.path.exists(lemma_outpath):\n",
    "        lemma_outpath_write = 'a'\n",
    "    else:\n",
    "        lemma_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(lemma_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_lemma)\n",
    "    f1 = open(lemma_outpath, lemma_outpath_write)\n",
    "    f1.writelines(results)\n",
    "    f1.close()       \n",
    "\n",
    "def spacy_no_stop(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ele.text.lower() for ele in doc if ((not ele.is_space) and (not ele.is_punct) and (not ele.is_stop))]\n",
    "    results = ' '.join(results) + '\\n'\n",
    "    return results\n",
    "def process_save_no_stop(texts, data_name, data_type, current_path):\n",
    "    no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_no_stop.txt\"\n",
    "    if os.path.exists(no_stop_outpath):\n",
    "        no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        no_stop_outpath_write = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(no_stop_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_no_stop)\n",
    "    f2 = open(no_stop_outpath, no_stop_outpath_write)\n",
    "    f2.writelines(results)\n",
    "    f2.close()\n",
    "    \n",
    "def spacy_lemma_no_stop(text):\n",
    "    doc = nlp(text)\n",
    "    results = [ele.lemma_.lower() for ele in doc if ((not ele.is_space) and (not ele.is_punct) and (not ele.is_stop))]\n",
    "    results = ' '.join(results) + '\\n'\n",
    "    return results\n",
    "def process_save_lemma_no_stop(texts, data_name, data_type, current_path):\n",
    "    lemma_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_lemma_no_stop.txt\"\n",
    "    if os.path.exists(lemma_no_stop_outpath):\n",
    "        lemma_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        lemma_no_stop_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(lemma_no_stop_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_lemma_no_stop)\n",
    "    f3 = open(lemma_no_stop_outpath, lemma_no_stop_outpath_write)\n",
    "    f3.writelines(results)\n",
    "    f3.close()\n",
    "\n",
    "def spacy_np_lemma(text):\n",
    "    doc = nlp(text.replace('\\n', ' '))\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.lemma_.lower().replace(' ', '_') for ele in doc if ((not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "def extract_noun_phrases_from_tok_list(a_list_of_toks):\n",
    "    results = [ele for ele in a_list_of_toks if '_' in ele]\n",
    "    return results\n",
    "def join_list_of_strings_add_newline(a_list_of_toks):\n",
    "    result = ' '.join(a_list_of_toks) + '\\n'\n",
    "    return result\n",
    "def process_save_np_lemma(texts, data_name, data_type, current_path):\n",
    "    np_lemma_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma.txt\"\n",
    "    if os.path.exists(np_lemma_outpath):\n",
    "        np_lemma_outpath_write = 'a'\n",
    "    else:\n",
    "        np_lemma_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_np_lemma)\n",
    "    result1 = parallel_apply_list(results, join_list_of_strings_add_newline) \n",
    "    f5 = open(np_lemma_outpath, np_lemma_outpath_write)\n",
    "    f5.writelines(result1)\n",
    "    f5.close()\n",
    "    \n",
    "    np_lemma_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_only.txt\"\n",
    "    if os.path.exists(np_lemma_outpath_only):\n",
    "        np_lemma_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_lemma_outpath_write_only = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_outpath_only), exist_ok=True)\n",
    "    result2 = parallel_apply_list(results, extract_noun_phrases_from_tok_list)\n",
    "    result2 = [' '.join(ele) + '\\n' for ele in result2]\n",
    "    f8 = open(np_lemma_outpath_only, np_lemma_outpath_write_only)\n",
    "    f8.writelines(result2)\n",
    "    f8.close()\n",
    "    \n",
    "def spacy_np_no_stop(text):\n",
    "    doc = nlp(text.replace('\\n', ' '))\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.text.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.text.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.text.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.text.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.text.lower().strip().replace(' ', '_') for ele in doc if ((not ele.is_stop) and (not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "def process_save_np_no_stop(texts, data_name, data_type, current_path):\n",
    "    np_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_no_stop.txt\"\n",
    "    if os.path.exists(np_no_stop_outpath):\n",
    "        np_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        np_no_stop_outpath_write = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(np_no_stop_outpath), exist_ok=True)\n",
    "    start_time = time.time()\n",
    "    results = parallel_apply_list(texts, spacy_np_no_stop)\n",
    "    print(f\"        Finished spacy_np_no_stop, taking {int(time.time() - start_time)} seconds...\")\n",
    "    start_time = time.time()\n",
    "    result1 = parallel_apply_list(results, join_list_of_strings_add_newline) \n",
    "    print(f\"        Finished join_list_of_strings_add_newline, taking {int(time.time() - start_time)} seconds...\")\n",
    "    start_time = time.time()\n",
    "    f4 = open(np_no_stop_outpath, np_no_stop_outpath_write)\n",
    "    f4.writelines(result1)\n",
    "    f4.close()\n",
    "    print(f\"        Finished writing to file, taking {int(time.time() - start_time)} seconds...\")\n",
    "    \n",
    "    np_no_stop_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_no_stop_only.txt\"\n",
    "    if os.path.exists(np_no_stop_outpath_only):\n",
    "        np_no_stop_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_no_stop_outpath_write_only = 'w'\n",
    "        \n",
    "    os.makedirs(os.path.dirname(np_no_stop_outpath_only), exist_ok=True)\n",
    "    result2 = parallel_apply_list(results, extract_noun_phrases_from_tok_list)\n",
    "    result2 = [' '.join(ele) + '\\n' for ele in result2]\n",
    "    f7 = open(np_no_stop_outpath_only, np_no_stop_outpath_write_only)\n",
    "    f7.writelines(result2)\n",
    "    f7.close()\n",
    "     \n",
    "def spacy_np_lemma_no_stop(text):\n",
    "    doc = nlp(text.replace('\\n', ' '))\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.lemma_.lower().replace(' ', '_') for ele in doc if ((not ele.is_stop) and (not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    tokenized_doc = ' '.join(tokenized_doc) + '\\n'\n",
    "    return tokenized_doc\n",
    "def process_save_np_lemma_no_stop(texts, data_name, data_type, current_path):\n",
    "    np_lemma_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_no_stop.txt\"\n",
    "    if os.path.exists(np_lemma_no_stop_outpath):\n",
    "        np_lemma_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        np_lemma_no_stop_outpath_write = 'w'\n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_no_stop_outpath), exist_ok=True)\n",
    "    results = parallel_apply_list(texts, spacy_np_lemma_no_stop)  \n",
    "    f6 = open(np_lemma_no_stop_outpath, np_lemma_no_stop_outpath_write)           \n",
    "    f6.writelines(results)\n",
    "    f6.close()\n",
    "    \n",
    "    \n",
    "def fully_process_partitions(partition, data_name='20_newgroups', data_type='train', current_path=current_path):\n",
    "    print(\"    Processing norm...\")\n",
    "    process_save_norm(texts=partition,\n",
    "                      data_name=data_name,\n",
    "                      data_type=data_type,\n",
    "                      current_path=current_path)\n",
    "    print(\"    Processing lemma...\")\n",
    "    process_save_lemma(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing no_stop...\")\n",
    "    process_save_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing lemma_no_stop...\")\n",
    "    process_save_lemma_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing np_lemma...\")\n",
    "    process_save_np_lemma(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing np_no_stop...\")\n",
    "    process_save_np_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)\n",
    "    print(\"    Processing np_lemma_no_stop...\")\n",
    "    process_save_np_lemma_no_stop(texts=partition,\n",
    "                       data_name=data_name,\n",
    "                       data_type=data_type,\n",
    "                       current_path=current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T23:40:57.469829Z",
     "start_time": "2020-07-24T23:40:57.384127Z"
    }
   },
   "outputs": [],
   "source": [
    "newgroups_train_data = pickle.load(open(newsgroups_train_data_loc, \"rb\"))\n",
    "newgroups_test_data = pickle.load(open(newsgroups_test_data_loc, \"rb\"))\n",
    "\n",
    "newgroups_train_data = [ele if len(ele) > 999998 else ele[:999998] for ele in newgroups_train_data]\n",
    "newgroups_test_data = [ele if len(ele) > 999998 else ele[:999998] for ele in newgroups_test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:06:01.241530Z",
     "start_time": "2020-07-16T13:47:57.715367Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting partition 0\n",
      "    Processing norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:32<00:00, 153.90it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:34<00:00, 143.32it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:37<00:00, 133.59it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:34<00:00, 143.95it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:10<00:00, 70.61it/s] \n",
      "100%|██████████| 5000/5000 [00:00<00:00, 15535.58it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 15530.80it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:02<00:00, 80.60it/s] \n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished spacy_np_no_stop, taking 102 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 21740.28it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished join_list_of_strings_add_newline, taking 0 seconds...\n",
      "        Finished writing to file, taking 0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 21880.78it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:02<00:00, 79.89it/s] \n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting partition 1\n",
      "    Processing norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:36<00:00, 137.12it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:42<00:00, 117.18it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:43<00:00, 114.80it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:41<00:00, 120.90it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:19<00:00, 62.55it/s] \n",
      "100%|██████████| 5000/5000 [00:00<00:00, 17527.43it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 18944.50it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:15<00:00, 65.99it/s] \n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished spacy_np_no_stop, taking 117 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 22615.73it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished join_list_of_strings_add_newline, taking 0 seconds...\n",
      "        Finished writing to file, taking 0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 23769.12it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:16<00:00, 65.70it/s] \n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting partition 2\n",
      "    Processing norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:08<00:00, 147.81it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:09<00:00, 131.54it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:10<00:00, 123.98it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:10<00:00, 121.71it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:10<00:00, 125.28it/s]\n",
      "100%|██████████| 1314/1314 [00:00<00:00, 5881.07it/s]\n",
      "100%|██████████| 1314/1314 [00:00<00:00, 6498.45it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:10<00:00, 121.99it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished spacy_np_no_stop, taking 12 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:00<00:00, 7164.66it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished join_list_of_strings_add_newline, taking 0 seconds...\n",
      "        Finished writing to file, taking 0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:00<00:00, 7638.07it/s]\n",
      "  0%|          | 0/1314 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1314/1314 [00:10<00:00, 126.29it/s]\n"
     ]
    }
   ],
   "source": [
    "partitions = minibatch(newgroups_train_data, size=5000)\n",
    "\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Starting partition {i}\")\n",
    "    fully_process_partitions(partition, data_name='20_newgroups', data_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T14:16:56.049402Z",
     "start_time": "2020-07-16T14:06:01.244332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting partition 0\n",
      "    Processing norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:38<00:00, 129.76it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:40<00:00, 124.91it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:39<00:00, 127.01it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:40<00:00, 123.45it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:14<00:00, 67.55it/s] \n",
      "100%|██████████| 5000/5000 [00:00<00:00, 17233.36it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 19073.98it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:10<00:00, 70.82it/s] \n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished spacy_np_no_stop, taking 90 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 23343.55it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished join_list_of_strings_add_newline, taking 0 seconds...\n",
      "        Finished writing to file, taking 0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 23624.13it/s]\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:08<00:00, 72.60it/s] \n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting partition 1\n",
      "    Processing norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:17<00:00, 148.38it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:20<00:00, 122.11it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:20<00:00, 122.17it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:21<00:00, 116.97it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:24<00:00, 101.84it/s]\n",
      "100%|██████████| 2532/2532 [00:00<00:00, 12612.46it/s]\n",
      "100%|██████████| 2532/2532 [00:00<00:00, 12728.92it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:19<00:00, 129.93it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished spacy_np_no_stop, taking 40 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:00<00:00, 13625.51it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Finished join_list_of_strings_add_newline, taking 0 seconds...\n",
      "        Finished writing to file, taking 0 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:00<00:00, 14923.10it/s]\n",
      "  0%|          | 0/2532 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processing np_lemma_no_stop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2532/2532 [00:19<00:00, 133.07it/s]\n"
     ]
    }
   ],
   "source": [
    "partitions = minibatch(newgroups_test_data, size=5000)\n",
    "\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Starting partition {i}\")\n",
    "    fully_process_partitions(partition, data_name='20_newgroups', data_type='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
