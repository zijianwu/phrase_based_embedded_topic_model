{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T01:54:34.378391Z",
     "start_time": "2020-07-25T01:54:34.376058Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T01:44:02.306500Z",
     "start_time": "2020-07-25T01:44:02.302091Z"
    }
   },
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "training_datasets = os.listdir(f\"{current_path}/../data/processed/wikipedia/\")\n",
    "training_datasets = [ele for ele in training_datasets if '.txt' in ele]\n",
    "\n",
    "windows = [3, 6, 9]\n",
    "epochs = [10, 20, 30, 40, 50]\n",
    "sg_types = [0,1]\n",
    "hs_types = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T01:46:11.006651Z",
     "start_time": "2020-07-25T01:46:10.996543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class for a memory-friendly iterator over the dataset\n",
    "class MySentences(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename):\n",
    "            yield line.split()\n",
    "\n",
    "\n",
    "def train_save_word2vec_model(dataset, min_count=2, size=50, window=3, workers=mp.cpu_count(), sg=1, hs=0, epochs=10):\n",
    "    sentences = MySentences(f\"{current_path}/../data/processed/wikipedia/{dataset}\")\n",
    "    model = gensim.models.Word2Vec(sentences=sentences,\n",
    "                                   min_count=min_count,\n",
    "                                   size=size,\n",
    "                                   window=window,\n",
    "                                   workers=workers,\n",
    "                                   sg=sg,  # 1 is sg, 0 is CBOW\n",
    "                                   hs=hs,  # 1 is hs, 0 is ns\n",
    "                                   iter=epochs\n",
    "                                   )\n",
    "    model.save(f\"{current_path}/../models/{dataset[:-4]}_word2vec_win{window}_sg{sg}_hs{hs}_epochs{epochs}.model\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_pretrained_embeddings(model, dataset, min_count=2, size=50, window=3, workers=mp.cpu_count(), sg=1, hs=0, epochs=10, model_type='word2vec'):\n",
    "    with open(f\"{current_path}/../pretrained_embeddings/{dataset[:-4]}_{model_type}_win{window}_sg{sg}_hs{hs}_epochs{epochs}.txt\", 'w') as f:\n",
    "        for v in list(model.wv.vocab):\n",
    "            vec = list(model.wv.__getitem__(v))\n",
    "            f.write(v + ' ')\n",
    "            vec_str = ['%.9f' % val for val in vec]\n",
    "            vec_str = \" \".join(vec_str)\n",
    "            f.write(vec_str + '\\n')\n",
    "            \n",
    "            \n",
    "def process_word2vec(dataset, min_count=2, size=50, window=3, workers=mp.cpu_count(), sg=1, hs=0, epochs=10):\n",
    "    model = train_save_word2vec_model(dataset, min_count, size, window, workers, sg, hs, epochs)\n",
    "    save_pretrained_embeddings(model, dataset, min_count, size, window, workers, sg, hs, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T01:47:05.844075Z",
     "start_time": "2020-07-25T01:47:05.842054Z"
    }
   },
   "outputs": [],
   "source": [
    "for dataset in training_datasets:\n",
    "    for window in windows:\n",
    "        for epoch in epochs:\n",
    "            for sg in sg_types:\n",
    "                for hs in hs_types:\n",
    "                    print(f\"Starting dataset|{dataset} window|{window} epoch|{epoch} sg|{sg} hs|{hs}\")\n",
    "                    start_time = time.time()\n",
    "                    process_word2vec(dataset, min_count=2, size=50, window=window, workers=mp.cpu_count(), sg=sg, hs=hs, epochs=epochs)\n",
    "                    print(f\"    Training took {int((time.time() - start_time) / 60)} min and {int((time.time() - start_time) % 60)} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_fasttext_model(dataset, min_count=2, size=50, window=3, workers=mp.cpu_count(), sg=1, hs=0, epochs=10):\n",
    "    sentences = MySentences(f\"{current_path}/../data/processed/wikipedia/{dataset}\")\n",
    "    model = gensim.models.FastText(size=size,\n",
    "                                   window=window,\n",
    "                                   workers = workers,\n",
    "                                   min_count = min_count,\n",
    "                                   sg = 1,\n",
    "                                   hs = 0,\n",
    "                                   iter = epochs)\n",
    "    model.build_vocab(sentences=sentences)\n",
    "    model.train(sentences=sentences, total_examples=len(sentences), epochs=epochs)  # train\n",
    "    model.save(f\"{current_path}/../models/{dataset[:-4]}_fasttext_win{window}_sg{sg}_hs{hs}_epochs{epochs}.model\")\n",
    "    \n",
    "def process_fasttext(dataset, min_count=2, size=50, window=3, workers=mp.cpu_count(), sg=1, hs=0, epochs=10):\n",
    "    model = train_save_fasttext_model(dataset, min_count, size, window, workers, sg, hs, epochs)\n",
    "    save_pretrained_embeddings(model, dataset, min_count, size, window, workers, sg, hs, epochs, model_type='fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in training_datasets:\n",
    "    for window in windows:\n",
    "        for epoch in epochs:\n",
    "            for sg in sg_types:\n",
    "                for hs in hs_types:\n",
    "                    print(f\"Starting dataset|{dataset} window|{window} epoch|{epoch} sg|{sg} hs|{hs}\")\n",
    "                    start_time = time.time()\n",
    "                    process_fasttext(dataset, min_count=2, size=50, window=window, workers=mp.cpu_count(), sg=sg, hs=hs, epochs=epochs)\n",
    "                    print(f\"    Training took {int((time.time() - start_time) / 60)} min and {int((time.time() - start_time) % 60)} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
