{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T03:06:00.139143Z",
     "start_time": "2020-07-14T03:05:55.306596Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from spacy.util import minibatch\n",
    "from spacy.attrs import LEMMA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[  # \"tagger\",\n",
    "    # \"parser\",\n",
    "    # \"ner\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T03:06:00.144275Z",
     "start_time": "2020-07-14T03:06:00.141394Z"
    }
   },
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "newsgroups_train_data_loc = f\"{current_path}/../data/raw/20_newsgroups/train_data.pkl\"\n",
    "newsgroups_test_data_loc = f\"{current_path}/../data/raw/20_newsgroups/test_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T03:06:00.221566Z",
     "start_time": "2020-07-14T03:06:00.146754Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def parallel_apply_list(a_list, a_function, n_jobs=mp.cpu_count(), func_param=None, n_threads=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies a_function to a_list using multiprocessing with n_jobs. If a_function has a specific\n",
    "    parameter that elements in a_list should fill, indicate it with func_param. If there are\n",
    "    other parameters in a_function that should be statically filled, use **kwargs.\n",
    "\n",
    "    If elements in a_list are tuples, lists, or anything else that provides multiple inputs\n",
    "    to a_function, wrap a_function so that it takes the entire tuple or list (see parallel_apply_row for example)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a_list : list\n",
    "    a_function : function object\n",
    "    n_jobs : int (multiprocessing)\n",
    "    n_threads : int (threading)\n",
    "    func_param : None (defaults to first parameter in function) or str (parameter in a_function)\n",
    "    kwargs : static keyword arguments to be given to all instances of a_function\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "    \"\"\"\n",
    "    if n_jobs:\n",
    "        executor = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\", prefer=\"processes\")\n",
    "    else:\n",
    "        executor = Parallel(n_jobs=n_threads, backend=\"threading\", prefer=\"threads\")\n",
    "    do = delayed(partial(a_function, **kwargs))\n",
    "    if func_param:\n",
    "        tasks = (do(**{func_param: ele}) for ele in tqdm(a_list))\n",
    "    else:\n",
    "        tasks = (do(ele) for ele in tqdm(a_list))\n",
    "    result = executor(tasks)\n",
    "    return result\n",
    "\n",
    "\n",
    "def spacy_np_lemma_no_stop(doc):\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.lemma_.lower().replace(' ', '_') for ele in doc if ((not ele.is_stop) and (not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "\n",
    "             \n",
    "def spacy_np_lemma(doc):\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.lemma_.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.lemma_.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.lemma_.lower().replace(' ', '_') for ele in doc if ((not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "\n",
    "                \n",
    "def spacy_np_no_stop(doc):\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        if len(np) > 1:\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                doc.vocab['_'.join([ele.text.lower() for ele in np])]\n",
    "                retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.text.lower() for ele in np])]})\n",
    "        for ent in doc.ents:\n",
    "            if len(ent) > 1:\n",
    "                with doc.retokenize() as retokenizer:\n",
    "                    doc.vocab['_'.join([ele.text.lower() for ele in np])]\n",
    "                    retokenizer.merge(np, attrs={LEMMA: doc.vocab.strings['_'.join([ele.text.lower() for ele in np])]})\n",
    "\n",
    "    tokenized_doc = [ele.text.lower().strip().replace(' ', '_') for ele in doc if ((not ele.is_stop) and (not ele.is_space) and (not ele.is_punct))]\n",
    "    tokenized_doc = [ele for ele in tokenized_doc if ele]\n",
    "    return tokenized_doc\n",
    "\n",
    "\n",
    "def spacy_process_save_text_batch(batch, data_name, data_type, current_path, return_obj):\n",
    "    norm_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_norm.txt\"\n",
    "    lemma_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_lemma.txt\"\n",
    "    no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_no_stop.txt\"\n",
    "    lemma_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_lemma_no_stop.txt\"\n",
    "    np_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_no_stop.txt\"\n",
    "    np_lemma_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma.txt\"\n",
    "    np_lemma_no_stop_outpath = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_no_stop.txt\"\n",
    "    np_no_stop_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_no_stop_only.txt\"\n",
    "    np_lemma_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_only.txt\"\n",
    "    np_lemma_no_stop_outpath_only = f\"{current_path}/../data/processed/{data_name}/{data_type}_np_lemma_no_stop_only.txt\"\n",
    "    \n",
    "    if os.path.exists(norm_outpath):\n",
    "        norm_outpath_write = 'a'\n",
    "    else:\n",
    "        norm_outpath_write = 'w'\n",
    "    if os.path.exists(lemma_outpath):\n",
    "        lemma_outpath_write = 'a'\n",
    "    else:\n",
    "        lemma_outpath_write = 'w'\n",
    "    if os.path.exists(no_stop_outpath):\n",
    "        no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        no_stop_outpath_write = 'w'\n",
    "    if os.path.exists(lemma_no_stop_outpath):\n",
    "        lemma_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        lemma_no_stop_outpath_write = 'w'\n",
    "    if os.path.exists(np_no_stop_outpath):\n",
    "        np_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        np_no_stop_outpath_write = 'w'\n",
    "    if os.path.exists(np_lemma_outpath):\n",
    "        np_lemma_outpath_write = 'a'\n",
    "    else:\n",
    "        np_lemma_outpath_write = 'w'\n",
    "    if os.path.exists(np_lemma_no_stop_outpath):\n",
    "        np_lemma_no_stop_outpath_write = 'a'\n",
    "    else:\n",
    "        np_lemma_no_stop_outpath_write = 'w'\n",
    "    if os.path.exists(np_no_stop_outpath_only):\n",
    "        np_no_stop_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_no_stop_outpath_write_only = 'w'\n",
    "    if os.path.exists(np_lemma_outpath_only):\n",
    "        np_lemma_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_lemma_outpath_write_only = 'w'\n",
    "    if os.path.exists(np_lemma_no_stop_outpath_only):\n",
    "        np_lemma_no_stop_outpath_write_only = 'a'\n",
    "    else:\n",
    "        np_lemma_no_stop_outpath_write_only = 'w'\n",
    "    \n",
    "    \n",
    "    os.makedirs(os.path.dirname(np_lemma_no_stop_outpath_only), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(np_lemma_outpath_only), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(np_no_stop_outpath_only), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(np_lemma_no_stop_outpath), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(np_lemma_outpath), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(np_no_stop_outpath), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(lemma_no_stop_outpath), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(no_stop_outpath), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(lemma_outpath), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(norm_outpath), exist_ok=True)\n",
    "    f0 = open(norm_outpath, norm_outpath_write)\n",
    "    f1 = open(lemma_outpath, lemma_outpath_write)\n",
    "    f2 = open(no_stop_outpath, no_stop_outpath_write)\n",
    "    f3 = open(lemma_no_stop_outpath, lemma_no_stop_outpath_write)\n",
    "    f4 = open(np_no_stop_outpath, np_no_stop_outpath_write)\n",
    "    f5 = open(np_lemma_outpath, np_lemma_outpath_write)\n",
    "    f6 = open(np_lemma_no_stop_outpath, np_lemma_no_stop_outpath_write)\n",
    "    f7 = open(np_no_stop_outpath_only, np_no_stop_outpath_write_only)\n",
    "    f8 = open(np_lemma_outpath_only, np_lemma_outpath_write_only)\n",
    "    f9 = open(np_lemma_no_stop_outpath_only, np_lemma_no_stop_outpath_write_only)\n",
    "    for doc in nlp.pipe(batch):\n",
    "        doc_norm = [ele.text.lower() for ele in doc if (not ele.is_space) and (not ele.is_punct)]\n",
    "        f0.write(\" \".join(doc_norm))\n",
    "        f0.write(\"\\n\")\n",
    "                \n",
    "        doc_lemma = [ele.lemma_.lower() for ele in doc if (not ele.is_space) and (not ele.is_punct)]\n",
    "        f1.write(\" \".join(doc_lemma))\n",
    "        f1.write(\"\\n\")\n",
    "                \n",
    "        doc_no_stop = [ele.text.lower() for ele in doc if ((not ele.is_space) and (not ele.is_punct) and (not ele.is_stop))]\n",
    "        f2.write(\" \".join(doc_no_stop))\n",
    "        f2.write(\"\\n\")\n",
    "                \n",
    "        doc_lemma_no_stop = [ele.lemma_.lower() for ele in doc if ((not ele.is_space) and (not ele.is_punct) and (not ele.is_stop))]\n",
    "        f3.write(\" \".join(doc_lemma_no_stop))\n",
    "        f3.write(\"\\n\")\n",
    "                \n",
    "        doc_np_no_stop = spacy_np_no_stop(doc)\n",
    "        f4.write(\" \".join(doc_np_no_stop))\n",
    "        f4.write(\"\\n\")\n",
    "                \n",
    "        doc_np_lemma = spacy_np_lemma(doc)\n",
    "        f5.write(\" \".join(doc_np_lemma))\n",
    "        f5.write(\"\\n\")\n",
    "                \n",
    "        doc_np_lemma_no_stop = spacy_np_lemma_no_stop(doc)\n",
    "        f6.write(\" \".join(doc_np_lemma_no_stop))\n",
    "        f6.write(\"\\n\")\n",
    "                \n",
    "        doc_np_no_stop_only = [ele for ele in doc_np_no_stop if '_' in ele]\n",
    "        f7.write(\" \".join(doc_np_no_stop_only))\n",
    "        f7.write(\"\\n\")\n",
    "                \n",
    "        doc_np_lemma_only = [ele for ele in doc_np_lemma if '_' in ele]\n",
    "        f8.write(\" \".join(doc_np_lemma_only))\n",
    "        f8.write(\"\\n\")\n",
    "                \n",
    "        doc_np_lemma_no_stop_only = [ele for ele in doc_np_lemma_no_stop if '_' in ele]\n",
    "        f9.write(\" \".join(doc_np_lemma_no_stop_only))\n",
    "        f9.write(\"\\n\")\n",
    "        \n",
    "    f0.close()\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    f4.close()\n",
    "    f5.close()\n",
    "    f6.close()\n",
    "    f7.close()\n",
    "    f8.close()\n",
    "    f9.close()\n",
    "    if return_obj:           \n",
    "        return (doc_norm,\n",
    "                doc_lemma,\n",
    "                doc_no_stop,\n",
    "                doc_lemma_no_stop,\n",
    "                doc_np_no_stop,\n",
    "                doc_np_lemma,\n",
    "                doc_np_lemma_no_stop,\n",
    "                doc_np_no_stop_only,\n",
    "                doc_np_lemma_only,\n",
    "                doc_np_lemma_no_stop_only)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def multiprocess_spacy_nlp(texts, batch_size=1000, n_jobs=mp.cpu_count(), data_name='20_newsgroups', data_type='train', current_path=os.path.dirname(os.path.abspath(\"__file__\")), return_obj=False):\n",
    "    partitions = minibatch(texts, size=batch_size)\n",
    "    results = parallel_apply_list(partitions, \n",
    "                                  spacy_process_save_text_batch,\n",
    "                                  data_name=data_name,\n",
    "                                  data_type=data_type,\n",
    "                                  current_path=current_path,\n",
    "                                  return_obj=return_obj,\n",
    "                                  n_jobs=n_jobs)\n",
    "    if return_obj:\n",
    "        doc_norm = [ele[0] for ele in results]\n",
    "        doc_lemma = [ele[1] for ele in results]\n",
    "        doc_no_stop = [ele[2] for ele in results]\n",
    "        doc_lemma_no_stop = [ele[3] for ele in results]\n",
    "        doc_np_no_stop = [ele[4] for ele in results]\n",
    "        doc_np_lemma = [ele[5] for ele in results]\n",
    "        doc_np_lemma_no_stop = [ele[6] for ele in results]\n",
    "        doc_np_no_stop_only = [ele[7] for ele in results]\n",
    "        doc_np_lemma_only = [ele[8] for ele in results]\n",
    "        doc_np_lemma_no_stop_only = [ele[9] for ele in results]\n",
    "        \n",
    "        doc_norm = [ele for sublist in doc_norm for ele in sublist]\n",
    "        doc_lemma = [ele for sublist in doc_lemma for ele in sublist]\n",
    "        doc_no_stop = [ele for sublist in doc_no_stop for ele in sublist]\n",
    "        doc_lemma_no_stop = [ele for sublist in doc_lemma_no_stop for ele in sublist]\n",
    "        doc_np_no_stop = [ele for sublist in doc_np_no_stop for ele in sublist]\n",
    "        doc_np_lemma = [ele for sublist in doc_np_lemma for ele in sublist]\n",
    "        doc_np_lemma_no_stop = [ele for sublist in doc_np_lemma_no_stop for ele in sublist]\n",
    "        doc_np_no_stop_only = [ele for sublist in doc_np_no_stop_only for ele in sublist]\n",
    "        doc_np_lemma_only = [ele for sublist in doc_np_lemma_only for ele in sublist]\n",
    "        doc_np_lemma_no_stop_only = [ele for sublist in doc_np_lemma_no_stop_only for ele in sublist]\n",
    "        \n",
    "        return (doc_norm,\n",
    "                doc_lemma,\n",
    "                doc_no_stop,\n",
    "                doc_lemma_no_stop,\n",
    "                doc_np_no_stop,\n",
    "                doc_np_lemma,\n",
    "                doc_np_lemma_no_stop,\n",
    "                doc_np_no_stop_only,\n",
    "                doc_np_lemma_only,\n",
    "                doc_np_lemma_no_stop_only)\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T03:06:00.287923Z",
     "start_time": "2020-07-14T03:06:00.223591Z"
    }
   },
   "outputs": [],
   "source": [
    "newgroups_train_data = pickle.load(open(newsgroups_train_data_loc, \"rb\"))\n",
    "newgroups_test_data = pickle.load(open(newsgroups_test_data_loc, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T16:52:27.535569Z",
     "start_time": "2020-07-14T03:06:00.289574Z"
    }
   },
   "outputs": [],
   "source": [
    "multiprocess_spacy_nlp(newgroups_train_data,\n",
    "                       batch_size=300,\n",
    "                       n_jobs=11,\n",
    "                       data_name='20_newsgroups',\n",
    "                       data_type='train',\n",
    "                       current_path=os.path.dirname(os.path.abspath(\"__file__\")),\n",
    "                       return_obj=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-14T17:47:27.523000Z",
     "start_time": "2020-07-14T16:52:27.538962Z"
    }
   },
   "outputs": [],
   "source": [
    "multiprocess_spacy_nlp(newgroups_test_data,\n",
    "                       batch_size=300,\n",
    "                       n_jobs=11,\n",
    "                       data_name='20_newsgroups',\n",
    "                       data_type='test',\n",
    "                       current_path=os.path.dirname(os.path.abspath(\"__file__\")),\n",
    "                       return_obj=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
